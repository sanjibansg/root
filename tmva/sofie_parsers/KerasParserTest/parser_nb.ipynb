{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed3fac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ....builtins/pyroot/pythonnizations/python/ROOT import pythonization\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import ROOT\n",
    "from ROOT import TMVA\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "def MakeKerasIdentity(layer):\n",
    "    input = layer['layerInput']\n",
    "    output = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = input[0]\n",
    "    fLayerOutputName = output[0]\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Identity('float')(fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Identity does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "def MakeKerasBinary(layer):\n",
    "    input = layer['layerInput']\n",
    "    output = layer['layerOutput']\n",
    "    fLayerType = layer['layerType'] \n",
    "    fLayerDType = layer['layerDType'] \n",
    "    fX1 = input[0]\n",
    "    fX2 = input[1]\n",
    "    fY = output[0]\n",
    "    op = None\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        if fLayerType == \"Add\":\n",
    "          op =  ROOT.TMVA.Experimental.SOFIE.ROperator_BasicBinary(float,'TMVA::Experimental::SOFIE::EBasicBinaryOperator::Add')(fX1, fX2, fY)\n",
    "        elif fLayerType == \"Subtract\":\n",
    "          op =  ROOT.TMVA.Experimental.SOFIE.ROperator_BasicBinary(float,'Sub')(fX1, fX2, fY)\n",
    "        else:\n",
    "          op =  ROOT.TMVA.Experimental.SOFIE.ROperator_BasicBinary(float,'Mul')(fX1, fX2, fY)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Identity does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "    return op\n",
    "\n",
    "\n",
    "def MakeKerasConcat(layer):\n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    attributes = layer['layerAttributes']\n",
    "    input = [str(i) for i in finput]\n",
    "    output = str(foutput[0])\n",
    "    axis = int(attributes[\"axis\"])\n",
    "    op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Concat('float')(input, axis, 0,  output)\n",
    "    return op\n",
    "\n",
    "def MakeKerasReshape(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible reshaping operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible reshaping operation using the SOFIE framework. Assumes layerDtype is float.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  name, data type, and other relevant information.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Reshape: A SOFIE framework operator representing the reshaping operation.\n",
    "    \"\"\"\n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    attributes = layer['layerAttributes']\n",
    "    flayername = attributes['_name']\n",
    "    fOpMode = ROOT.TMVA.Experimental.SOFIE.ReshapeOpMode.Reshape\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fNameData = finput[0]\n",
    "    fNameOutput = foutput[0]\n",
    "    fNameShape = flayername + \"ReshapeAxes\"\n",
    "    op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Reshape(fOpMode, 0, fNameData, fNameShape, fNameOutput)\n",
    "    return op\n",
    "\n",
    "def MakeKerasFlatten(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible flattening operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible flattening operation using the SOFIE framework.\n",
    "    Flattening is the process of converting a multi-dimensional tensor into a\n",
    "    one-dimensional tensor. Assumes layerDtype is float.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                name, data type, and other relevant information.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Reshape: A SOFIE framework operator representing the flattening operation.\n",
    "    \"\"\"\n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    attributes = layer['layerAttributes']\n",
    "    flayername = attributes['_name']\n",
    "    fOpMode = ROOT.TMVA.Experimental.SOFIE.ReshapeOpMode.Flatten\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fNameData = finput[0]\n",
    "    fNameOutput = foutput[0]\n",
    "    fNameShape = flayername + \"ReshapeAxes\"\n",
    "    op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Reshape(fOpMode, 0, fNameData, fNameShape, fNameOutput)\n",
    "    return op\n",
    "\n",
    "\n",
    "def MakeKerasBatchNorm(layer): \n",
    "    \"\"\"\n",
    "    Create a Keras-compatible batch normalization operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a batch normalization layer and its\n",
    "    attributes and constructs a Keras-compatible batch normalization operation using\n",
    "    the SOFIE framework. Batch normalization is used to normalize the activations of\n",
    "    a neural network, typically applied after the convolutional or dense layers.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  gamma, beta, moving mean, moving variance, epsilon,\n",
    "                  momentum, data type (assumed to be float), and other relevant information.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_BatchNormalization: A SOFIE framework operator representing the batch normalization operation.\n",
    "    \"\"\"\n",
    "        \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    attributes = layer['layerAttributes']\n",
    "    gamma = attributes[\"gamma\"]\n",
    "    beta = attributes[\"beta\"]\n",
    "    moving_mean = attributes[\"moving_mean\"]\n",
    "    moving_variance = attributes[\"moving_variance\"]\n",
    "    fLayerDType = layer[\"layerDType\"]\n",
    "    fNX = str(finput[0])\n",
    "    fNY = str(foutput[0])\n",
    "    fNScale = str(gamma.name)\n",
    "    fNB = str(beta.name)\n",
    "    fNMean = str(moving_mean.name)\n",
    "    fNVar = str(moving_variance.name)\n",
    "    epsilon = attributes[\"epsilon\"]\n",
    "    momentum = attributes[\"momentum\"]\n",
    "    op =  ROOT.TMVA.Experimental.SOFIE.ROperator_BatchNormalization('float')(epsilon, momentum, 0, fNX, fNScale, fNB, fNMean, fNVar, fNY)\n",
    "    return op\n",
    "\n",
    "def MakeKerasActivation(layer):\n",
    "    attributes = layer['layerAttributes']\n",
    "    activation = attributes['activation']\n",
    "    if hasattr(activation, '__name__'):\n",
    "        fLayerActivation = str(activation.__name__)\n",
    "    else:\n",
    "        fLayerActivation = str(activation.__class__.__name__)\n",
    "        \n",
    "    if fLayerActivation in mapKerasLayer.keys():\n",
    "        return mapKerasLayer[fLayerActivation](layer)\n",
    "    else:\n",
    "        raise Exception(\"TMVA.SOFIE - parsing keras activation layer \" + fLayerActivation + \" is not yet supported\")\n",
    "\n",
    "def MakeKerasReLU(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible rectified linear unit (ReLU) activation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible ReLU activation operation using the SOFIE framework.\n",
    "    ReLU is a popular activation function that replaces all negative values in a tensor\n",
    "    with zero, while leaving positive values unchanged.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  and data type, which must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Relu: A SOFIE framework operator representing the ReLU activation operation.\n",
    "    \"\"\"\n",
    "        \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Relu('float')(fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Relu does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasSeLU(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible scaled exponential linear unit (SeLU) activation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible SeLU activation operation using the SOFIE framework.\n",
    "    SeLU is a type of activation function that introduces self-normalizing properties\n",
    "    to the neural network.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  and data type - must be float32.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Selu: A SOFIE framework operator representing the SeLU activation operation.\n",
    "    \"\"\"\n",
    "        \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Selu('float')(fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Selu does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasSigmoid(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible sigmoid activation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible sigmoid activation operation using the SOFIE framework.\n",
    "    Sigmoid is a commonly used activation function that maps input values to the range\n",
    "    between 0 and 1, providing a way to introduce non-linearity in neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  and data type - must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Sigmoid: A SOFIE framework operator representing the sigmoid activation operation.\n",
    "    \"\"\"\n",
    "        \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Sigmoid('float')(fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Sigmoid does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasSoftmax(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible softmax activation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible softmax activation operation using the SOFIE framework.\n",
    "    Softmax is an activation function that converts input values into a probability\n",
    "    distribution, often used in the output layer of a neural network for multi-class\n",
    "    classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  and data type - must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Softmax: A SOFIE framework operator representing the softmax activation operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Softmax('float')(-1, fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Softmax does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasLeakyRelu(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible Leaky ReLU activation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible Leaky ReLU activation operation using the SOFIE framework.\n",
    "    Leaky ReLU is a variation of the ReLU activation function that allows small negative\n",
    "    values to pass through, introducing non-linearity while preventing \"dying\" neurons.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  attributes, and data type - must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_LeakyRelu: A SOFIE framework operator representing the Leaky ReLU activation operation.\n",
    "    \"\"\"\n",
    "        \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    attributes = layer['layerAttributes']\n",
    "    if hasattr(attributes, 'alpha'):\n",
    "        fAlpha = float(attributes[\"alpha\"])\n",
    "    else:\n",
    "        fAlpha = float(attributes['activation'].alpha)\n",
    "        \n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_LeakyRelu('float')(fAlpha, fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator LeakyRelu does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasTanh(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible hyperbolic tangent (tanh) activation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible tanh activation operation using the SOFIE framework.\n",
    "    Tanh is an activation function that squashes input values to the range between -1 and 1,\n",
    "    introducing non-linearity in neural networks.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  and data type - must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Tanh: A SOFIE framework operator representing the tanh activation operation.\n",
    "    \"\"\"\n",
    "        \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Tanh('float')(fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Tanh does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasSwish(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible swish activation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible swish activation operation using the SOFIE framework.\n",
    "    Swish is an activation function that aims to combine the benefits of ReLU and sigmoid,\n",
    "    allowing some non-linearity while still keeping positive values unbounded.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  and data type.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Swish: A SOFIE framework operator representing the swish activation operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Swish('float')(fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Swish does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasPermute(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible permutation operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a layer and its attributes and\n",
    "    constructs a Keras-compatible permutation operation using the SOFIE framework.\n",
    "    Permutation is an operation that rearranges the dimensions of a tensor based on\n",
    "    specified dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  attributes, and data type - must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Transpose: A SOFIE framework operator representing the permutation operation.\n",
    "    \"\"\"\n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    attributes = layer['layerAttributes']\n",
    "    fAttributePermute = np.asarray(attributes[\"dims\"])\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        if len(fAttributePermute) > 0:\n",
    "            op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Transpose('float')(ROOT.TMVA.Experimental.SOFIE.fPermuteDims, fLayerInputName, fLayerOutputName)\n",
    "        else:    \n",
    "            op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Transpose('float')(fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Transpose does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasDense(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible dense (fully connected) layer operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a dense layer and its attributes and\n",
    "    constructs a Keras-compatible dense (fully connected) layer operation using the SOFIE framework.\n",
    "    A dense layer applies a matrix multiplication between the input tensor and weight matrix,\n",
    "    and adds a bias term.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  layer weight names, and data type - must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Gemm: A SOFIE framework operator representing the dense layer operation.\n",
    "    \"\"\"  \n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    fWeightNames = layer[\"layerWeight\"]\n",
    "    fKernelName = fWeightNames[0]\n",
    "    fBiasName = fWeightNames[1]\n",
    "    attr_alpha = 1.0\n",
    "    attr_beta  = 1.0\n",
    "    attr_transA = 0\n",
    "    attr_transB = 0\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Gemm['float'](attr_alpha, attr_beta, attr_transA, attr_transB, fLayerInputName, fKernelName, fBiasName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Gemm does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasConv(layer): \n",
    "    \"\"\"\n",
    "    Create a Keras-compatible convolutional layer operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a convolutional layer and its attributes and\n",
    "    constructs a Keras-compatible convolutional layer operation using the SOFIE framework.\n",
    "    A convolutional layer applies a convolution operation between the input tensor and a set\n",
    "    of learnable filters (kernels).\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  data type (must be float), weight and bias name, kernel size, dilations, padding and strides. \n",
    "                  When padding is same (keep in the same dimensions), the padding shape is calculated.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Conv: A SOFIE framework operator representing the convolutional layer operation.\n",
    "    \"\"\"\n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerDType = layer['layerDType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    attributes = layer['layerAttributes']\n",
    "    fWeightNames = layer[\"layerWeight\"]\n",
    "    fKernelName = fWeightNames[0]\n",
    "    fBiasName = fWeightNames[1]\n",
    "    fAttrDilations = attributes[\"dilation_rate\"]\n",
    "    fAttrGroup = int(attributes[\"groups\"])\n",
    "    fAttrKernelShape = attributes[\"kernel_size\"]\n",
    "    fKerasPadding = str(attributes[\"padding\"])\n",
    "    fAttrStrides = attributes[\"strides\"]\n",
    "    fAttrPads = []\n",
    "    if fKerasPadding == 'valid':\n",
    "        fAttrAutopad = 'VALID'\n",
    "    elif fKerasPadding == 'same':\n",
    "        fAttrAutopad = 'NOTSET'\n",
    "        fInputShape = attributes['_build_input_shape']\n",
    "        inputHeight = fInputShape[1]\n",
    "        inputWidth = fInputShape[2]\n",
    "        outputHeight = math.ceil(float(inputHeight) / float(fAttrStrides[0]))\n",
    "        outputWidth = math.ceil(float(inputWidth) / float(fAttrStrides[1]))\n",
    "        padding_height = max((outputHeight - 1) * fAttrStrides[0] + fAttrKernelShape[0] - inputHeight, 0)\n",
    "        padding_width = max((outputWidth - 1) * fAttrStrides[1] + fAttrKernelShape[1] - inputWidth, 0)\n",
    "        padding_top = math.floor(padding_height / 2)\n",
    "        padding_bottom = padding_height - padding_top\n",
    "        padding_left = math.floor(padding_width / 2)\n",
    "        padding_right = padding_width - padding_left\n",
    "        fAttrPads = [padding_top, padding_bottom, padding_left, padding_right]\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - RModel Keras Parser doesn't yet supports Convolution layer with padding \" + fKerasPadding\n",
    "        )\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Conv['float'](fAttrAutopad, fAttrDilations, fAttrGroup, \n",
    "                                                                  fAttrKernelShape, fAttrPads, fAttrStrides, \n",
    "                                                                  fLayerInputName, fKernelName, fBiasName, \n",
    "                                                                  fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Gemm does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasPooling(layer):\n",
    "    \"\"\"\n",
    "    Create a Keras-compatible pooling layer operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing a pooling layer and its attributes and\n",
    "    constructs a Keras-compatible pooling layer operation using the SOFIE framework.\n",
    "    Pooling layers downsample the input tensor by selecting a representative value from\n",
    "    a group of neighboring values, either by taking the maximum or the average.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  layer type (the selection rule), the pool size, padding, strides, and data type.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_Pool: A SOFIE framework operator representing the pooling layer operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    #extract attributes from layer data\n",
    "    fLayerDType = layer['layerDType']\n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    fLayerType = layer['layerType']\n",
    "    fLayerInputName = finput[0]\n",
    "    fLayerOutputName = foutput[0]\n",
    "    pool_atrr = ROOT.TMVA.Experimental.SOFIE.RAttributes_Pool()\n",
    "    attributes = layer['layerAttributes']\n",
    "    fAttrKernelShape = attributes[\"pool_size\"]\n",
    "    fKerasPadding = str(attributes[\"padding\"])\n",
    "    fAttrStrides = attributes[\"strides\"]\n",
    "    \n",
    "    #Set default values\n",
    "    fAttrDilations = (1,1)\n",
    "    fpads = [0,0,0,0,0,0]\n",
    "    pool_atrr.ceil_mode = 0\n",
    "    pool_atrr.count_include_pad = 0\n",
    "    pool_atrr.storage_order = 0\n",
    "    \n",
    "    if fKerasPadding == 'valid':\n",
    "        fAttrAutopad = 'VALID'\n",
    "    elif fKerasPadding == 'same':\n",
    "        fAttrAutopad = 'NOTSET'\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - RModel Keras Parser doesn't yet supports Convolution layer with padding \" + fKerasPadding\n",
    "        )\n",
    "    pool_atrr.dilations = list(fAttrDilations)\n",
    "    pool_atrr.strides = list(fAttrStrides)\n",
    "    pool_atrr.pads = fpads\n",
    "    pool_atrr.kernel_shape = list(fAttrKernelShape)\n",
    "    pool_atrr.auto_pad = fAttrAutopad    \n",
    "    \n",
    "    #choose pooling type\n",
    "    if fLayerType.startswith(\"Max\"):\n",
    "        PoolMode =  ROOT.TMVA.Experimental.SOFIE.PoolOpMode.MaxPool\n",
    "    elif fLayerType.startswith(\"AveragePool\"):\n",
    "        PoolMode =  ROOT.TMVA.Experimental.SOFIE.PoolOpMode.AveragePool\n",
    "    elif fLayerType.startswith(\"GlobalAverage\"):\n",
    "        PoolMode =  ROOT.TMVA.Experimental.SOFIE.PoolOpMode.GloabalAveragePool\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator poolong does not yet support pooling type \" + fLayerType\n",
    "        )\n",
    "    \n",
    "    \n",
    "    #create operator\n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Pool['float'](PoolMode, pool_atrr, fLayerInputName, fLayerOutputName)\n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator Pooling does not yet support input type \" + fLayerDType\n",
    "        )\n",
    "\n",
    "\n",
    "def MakeKerasRNN(layer): \n",
    "    \"\"\"\n",
    "    Create a Keras-compatible RNN (Recurrent Neural Network) layer operation using SOFIE framework.\n",
    "\n",
    "    This function takes a dictionary representing an RNN layer and its attributes and\n",
    "    constructs a Keras-compatible RNN layer operation using the SOFIE framework.\n",
    "    RNN layers are used to model sequences, and they maintain internal states that are\n",
    "    updated through recurrent connections.\n",
    "\n",
    "    Parameters:\n",
    "    layer (dict): A dictionary containing layer information including input, output,\n",
    "                  layer type, attributes, weights, and data type - must be float.\n",
    "\n",
    "    Returns:\n",
    "    ROperator_RNN: A SOFIE framework operator representing the RNN layer operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract required information from the layer dictionary\n",
    "    fLayerDType = layer['layerDType']\n",
    "    finput = layer['layerInput']\n",
    "    foutput = layer['layerOutput']\n",
    "    attributes = layer['layerAttributes']\n",
    "    direction = attributes['direction']\n",
    "    hidden_size = attributes[\"hidden_size\"]\n",
    "    layout = int(attributes[\"layout\"])\n",
    "    nameX = finput[0]\n",
    "    nameY = foutput[0]\n",
    "    nameW = layer[\"layerWeight\"][0]\n",
    "    nameR = layer[\"layerWeight\"][1]\n",
    "    if len(layer[\"layerWeight\"]) > 2:\n",
    "        nameB = layer[\"layerWeight\"][2]\n",
    "    else:\n",
    "        nameB = \"\"\n",
    "    \n",
    "    # Check if the provided activation function is supported\n",
    "    fPActivation = attributes['activation']\n",
    "    if not fPActivation.__name__ in ['relu', 'sigmoid', 'tanh', 'softsign', 'softplus']: #avoiding functions with parameters\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator RNN does not yet support activation function \" + fPActivation.__name__\n",
    "        )\n",
    "    # print(fPActivation)\n",
    "    activations = [fPActivation.__name__[0].upper()+fPActivation.__name__[1:]]\n",
    "    print(activations)\n",
    "\n",
    "    #set default values\n",
    "    activation_alpha = []\n",
    "    activation_beta = []\n",
    "    clip = 0.0\n",
    "    nameY_h = \"\"\n",
    "    nameInitial_h = \"\"\n",
    "    name_seq_len = \"\"\n",
    "    \n",
    "    if  ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fLayerDType) ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "        if layer['layerType'] == \"SimpleRNN\":\n",
    "            op =  ROOT.TMVA.Experimental.SOFIE.ROperator_RNN['float'](activation_alpha, activation_beta, activations, clip, direction, hidden_size, layout, nameX, nameW, nameR, nameB, name_seq_len, nameInitial_h, nameY, nameY_h)\n",
    "        \n",
    "        elif layer['layerType'] == \"GRU\":\n",
    "            #an additional activation function is required, given by the user\n",
    "            activations.insert(0, attributes['recurrent_activation'].__name__[0].upper() + attributes['recurrent_activation'].__name__[1:])\n",
    "            \n",
    "            #new variable needed:\n",
    "            linear_before_reset = attributes['linear_before_reset']\n",
    "            print(activations[0])\n",
    "            op =  ROOT.TMVA.Experimental.SOFIE.ROperator_GRU['float'](activation_alpha, activation_beta, activations, clip, direction, hidden_size, layout, linear_before_reset, nameX, nameW, nameR, nameB, name_seq_len, nameInitial_h, nameY, nameY_h)\n",
    "        \n",
    "        elif layer['layerType'] == \"LSTM\":\n",
    "            #an additional activation function is required, the first given by the user, the second set to tanh as default\n",
    "            fPRecurrentActivation = attributes['recurrent_activation']\n",
    "            if not fPActivation.__name__ in ['relu', 'sigmoid', 'tanh', 'softsign', 'softplus']: #avoiding functions with parameters\n",
    "                raise RuntimeError(\n",
    "                    \"TMVA::SOFIE - Unsupported - Operator RNN does not yet support recurrent activation function \" + fPActivation.__name__\n",
    "                )\n",
    "            fPRecurrentActivationName = fPRecurrentActivation.__name__[0].upper()+fPRecurrentActivation.__name__[1:]\n",
    "            activations.insert(0,fPRecurrentActivationName)\n",
    "            activations.insert(2,'Tanh')            \n",
    "            \n",
    "            #new variables needed:\n",
    "            input_forget = 0\n",
    "            nameInitial_c = \"\"\n",
    "            nameP = \"\" #No peephole connections in keras LSTM model\n",
    "            nameY_c = \"\"\n",
    "            op =  ROOT.TMVA.Experimental.SOFIE.ROperator_LSTM['float'](activation_alpha, activation_beta, activations, clip, direction, hidden_size, input_forget, layout, nameX, nameW, nameR, nameB, name_seq_len, nameInitial_h, nameInitial_c, nameP, nameY, nameY_h, nameY_c)\n",
    "        \n",
    "        else: \n",
    "            raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator RNN does not yet support operator type \" + layer['layerType']\n",
    "        ) \n",
    "        return op\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"TMVA::SOFIE - Unsupported - Operator RNN does not yet support input type \" + fLayerDType\n",
    "        )   \n",
    "\n",
    "#Set global dictionaries, mapping layers to corresponding functions that create their ROperator instances\n",
    "mapKerasLayer = {\"Activation\": MakeKerasActivation,\n",
    "                 \"Permute\": MakeKerasPermute,\n",
    "                 \"BatchNormalization\": MakeKerasBatchNorm,\n",
    "                 \"Reshape\": MakeKerasReshape,\n",
    "                 \"Flatten\": MakeKerasFlatten,\n",
    "                 \"Concatenate\": MakeKerasConcat,\n",
    "                 \"swish\": MakeKerasSwish,\n",
    "                 \"Add\": MakeKerasBinary,\n",
    "                 \"Subtract\": MakeKerasBinary,\n",
    "                 \"Multiply\": MakeKerasBinary,\n",
    "                 \"Softmax\": MakeKerasSoftmax,\n",
    "                 \"tanh\": MakeKerasTanh,\n",
    "                 \"Identity\": MakeKerasIdentity,\n",
    "                 \"Dropout\": MakeKerasIdentity,\n",
    "                 \"ReLU\": MakeKerasReLU,\n",
    "                 \"relu\": MakeKerasReLU,\n",
    "                 \"selu\": MakeKerasSeLU,\n",
    "                 \"sigmoid\": MakeKerasSigmoid,\n",
    "                 \"LeakyReLU\": MakeKerasLeakyRelu, \n",
    "                 \"softmax\": MakeKerasSoftmax, \n",
    "                 \"MaxPooling2D\": MakeKerasPooling,\n",
    "                 \"SimpleRNN\": MakeKerasRNN,\n",
    "                 \"GRU\": MakeKerasRNN,\n",
    "                 \"LSTM\": MakeKerasRNN,\n",
    "                 }\n",
    "\n",
    "mapKerasLayerWithActivation = {\"Dense\": MakeKerasDense,\"Conv2D\": MakeKerasConv}\n",
    "\n",
    "\n",
    "def add_layer_into_RModel(rmodel, layer_data):\n",
    "    \"\"\"\n",
    "    Add a Keras layer operation to an existing RModel using the SOFIE framework.\n",
    "\n",
    "    This function takes an existing RModel and a dictionary representing a Keras layer\n",
    "    and its attributes, and adds the corresponding layer operation to the RModel using\n",
    "    the SOFIE framework. The function supports various types of Keras layers, including\n",
    "    those with or without activation functions.\n",
    "\n",
    "    Parameters:\n",
    "    rmodel (RModel): An existing RModel to which the layer operation will be added.\n",
    "    layer_data (dict): A dictionary containing layer information including type,\n",
    "                      attributes, input, output, and layer data type.\n",
    "\n",
    "    Returns:\n",
    "    RModel: The updated RModel after adding the layer operation.\n",
    "\n",
    "    Raises exception: If the provided layer type or activation function is not supported.\n",
    "    \"\"\"\n",
    "    \n",
    "    fLayerType = layer_data['layerType']\n",
    "    \n",
    "    #reshape and flatten layers don't have weights, but they are needed inside the list of initialized tensor list in the Rmodel\n",
    "    if fLayerType == \"Reshape\" or fLayerType == \"Flatten\":\n",
    "        Attributes = layer_data['layerAttributes']\n",
    "        LayerName = Attributes['_name']\n",
    "        if fLayerType == \"Reshape\":\n",
    "            TargetShape = np.asarray(Attributes['target_shape']).astype(\"int\")\n",
    "            TargetShape = np.insert(TargetShape,0,0)\n",
    "        else:\n",
    "            input_shape = layer_data['layerAttributes']['_build_input_shape']\n",
    "            TargetShape = [ ROOT.TMVA.Experimental.SOFIE.ConvertShapeToLength(input_shape[1:])]\n",
    "            TargetShape = np.asarray(TargetShape)\n",
    "        \n",
    "        #since the AddInitializedTensor method in RModel requires unique pointer, we call a helper function in c++ that does the conversion from a regular pointer to unique one in c++\n",
    "        rmodel.AddInitializedTensor['long'](LayerName+\"ReshapeAxes\", [len(TargetShape)], TargetShape)\n",
    "    \n",
    "    #These layers only have one operator - excluding the recurrent layers, in which the activation function(s) are included in the recurrent operator\n",
    "    if fLayerType in mapKerasLayer.keys():\n",
    "        Attribues = layer_data['layerAttributes']\n",
    "        inputs = layer_data['layerInput']\n",
    "        outputs = layer_data['layerOutput']\n",
    "        LayerName = Attribues['_name']\n",
    "        \n",
    "        #Pooling layers in keras by default assume the channels dimension is the last one, \n",
    "        #while in onnx (and the RModel) it is the first one (other than batch size), \n",
    "        #so a transpose is needed before and after the pooling, if the data format is channels last (can be set to channels first by the user).\n",
    "        if fLayerType == 'MaxPooling2D':\n",
    "            if layer_data['channels_last']:\n",
    "                op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Transpose('float')([0,3,1,2], inputs[0], LayerName+\"PreTrans\")\n",
    "                rmodel.AddOperatorReference(op)\n",
    "                inputs[0] = LayerName+\"PreTrans\"\n",
    "                layer_data[\"layerInput\"] = inputs\n",
    "                outputs[0] = LayerName+fLayerType\n",
    "                layer_data['layerOutput'] = outputs\n",
    "        rmodel.AddOperatorReference(mapKerasLayer[fLayerType](layer_data))\n",
    "        if fLayerType == 'MaxPooling2D':\n",
    "            if layer_data['channels_last']:\n",
    "                op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Transpose('float')([0,2,3,1], LayerName+fLayerType, LayerName+\"PostTrans\")\n",
    "                rmodel.AddOperatorReference(op)\n",
    "        return rmodel\n",
    "    \n",
    "    #These layers require two operators - dense/conv and their activation function\n",
    "    elif fLayerType in mapKerasLayerWithActivation.keys():\n",
    "        Attribues = layer_data['layerAttributes']\n",
    "        LayerName = Attribues['_name']\n",
    "        fPActivation = Attribues['activation']\n",
    "        LayerActivation = fPActivation.__name__\n",
    "        if LayerActivation in ['selu', 'sigmoid']:\n",
    "            rmodel.AddNeededStdLib(\"cmath\")\n",
    "        \n",
    "        #if there is an activation function after the layer\n",
    "        if LayerActivation != 'linear':\n",
    "            outputs = layer_data['layerOutput']\n",
    "            inputs = layer_data['layerInput']\n",
    "            fActivationLayerOutput = outputs[0]\n",
    "            \n",
    "            #like pooling, convolutional layer from keras requires transpose before and after to match the onnx format \n",
    "            # if the data format is channels last (can be set to channels first by the user).\n",
    "            if fLayerType == 'Conv2D':\n",
    "                if layer_data['channels_last']:\n",
    "                    op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Transpose('float')([0,3,1,2], inputs[0], LayerName+\"PreTrans\")\n",
    "                    rmodel.AddOperatorReference(op)\n",
    "                    inputs[0] = LayerName+\"PreTrans\"\n",
    "                    layer_data[\"layerInput\"] = inputs\n",
    "            outputs[0] = LayerName+fLayerType\n",
    "            layer_data['layerOutput'] = outputs\n",
    "            op = mapKerasLayerWithActivation[fLayerType](layer_data)\n",
    "            rmodel.AddOperatorReference(op)\n",
    "            Activation_layer_input = LayerName+fLayerType\n",
    "            if fLayerType == 'Conv2D':\n",
    "                if layer_data['channels_last']:\n",
    "                    op =  ROOT.TMVA.Experimental.SOFIE.ROperator_Transpose('float')([0,2,3,1], LayerName+fLayerType, LayerName+\"PostTrans\")\n",
    "                    rmodel.AddOperatorReference(op)\n",
    "                    Activation_layer_input = LayerName + \"PostTrans\"\n",
    "            \n",
    "            #Adding the activation function\n",
    "            inputs[0] = Activation_layer_input\n",
    "            outputs[0] = fActivationLayerOutput\n",
    "            layer_data['layerInput'] = inputs\n",
    "            layer_data['layerOutput'] = outputs\n",
    "            if not LayerActivation in mapKerasLayer.keys():\n",
    "                raise Exception(\"TMVA.SOFIE - parsing keras activation function \" + LayerActivation + \" is not yet supported\")\n",
    "            rmodel.AddOperatorReference(mapKerasLayer[LayerActivation](layer_data))\n",
    "            \n",
    "        else: #there is a bug here if it is conv and the activation is linear, need to add transpose before and after\n",
    "            rmodel.AddOperatorReference(mapKerasLayerWithActivation[fLayerType](layer_data))\n",
    "        return rmodel\n",
    "    else:\n",
    "        raise Exception(\"TMVA.SOFIE - parsing keras layer \" + fLayerType + \" is not yet supported\")\n",
    "\n",
    "\n",
    "class RModelParser_Keras:\n",
    "\n",
    "    def Parse(filename):\n",
    "        #Check if file exists\n",
    "        if not os.path.exists(filename):\n",
    "            raise RuntimeError(\"Model file {} not found!\".format(filename))\n",
    "            \n",
    "        #load model\n",
    "        keras_model = keras.models.load_model(filename)\n",
    "        keras_model.load_weights(filename)\n",
    "        \n",
    "        #create new RModel object\n",
    "        sep = '/'\n",
    "        if os.name == 'nt':\n",
    "            sep = '\\\\'\n",
    "        \n",
    "        isep = filename.rfind(sep)\n",
    "        filename_nodir = filename\n",
    "        if isep != -1:\n",
    "            filename_nodir = filename[isep+1:]\n",
    "        \n",
    "        ttime = time.time()\n",
    "        gmt_time = time.gmtime(ttime)\n",
    "        parsetime = time.asctime(gmt_time)\n",
    "        \n",
    "        rmodel =  ROOT.TMVA.Experimental.SOFIE.RModel.RModel(filename_nodir, parsetime)\n",
    "        \n",
    "        #iterate over the layers and add them to the RModel\n",
    "        for layer in keras_model.layers:\n",
    "            layer_data={}\n",
    "            layer_data['layerType']=layer.__class__.__name__\n",
    "            layer_data['layerAttributes']=layer.__dict__\n",
    "            layer_data['layerInput']=[x.name for x in layer.input] if isinstance(layer.input,list) else [layer.input.name]\n",
    "            layer_data['layerOutput']=[x.name for x in layer.output] if isinstance(layer.output,list) else [layer.output.name]\n",
    "            layer_data['layerDType']=layer.dtype\n",
    "            layer_data['layerWeight']=[x.name for x in layer.weights]\n",
    "            \n",
    "            #for convolutional and pooling layers we need to know the format of the data\n",
    "            if layer_data['layerType'] in ['Conv2D', 'MaxPooling2D']:\n",
    "                layer_data['channels_last'] = True if layer.data_format == 'channels_last' else False\n",
    "                \n",
    "            #for recurrent type layers we need to extract additional unique information\n",
    "            if layer_data['layerType'] in [\"SimpleRNN\", \"LSTM\", \"GRU\"]:\n",
    "                layer_data['layerAttributes']['activation'] = layer.activation\n",
    "                layer_data['layerAttributes']['direction'] = 'backward' if layer.go_backwards else 'forward'\n",
    "                layer_data['layerAttributes'][\"units\"] = layer.units\n",
    "                layer_data['layerAttributes'][\"layout\"] = layer.input.shape[0] is None\n",
    "                layer_data['layerAttributes'][\"hidden_size\"] = layer.output.shape[-1]\n",
    "                \n",
    "                #for GRU and LSTM we need to extract an additional activation function\n",
    "                if layer_data['layerType'] != \"SimpleRNN\": \n",
    "                    layer_data['layerAttributes']['recurrent_activation'] = layer.recurrent_activation\n",
    "                \n",
    "                #for GRU there are two variants of the reset gate location, we need to know which one is it\n",
    "                if layer_data['layerType'] == \"GRU\":\n",
    "                    layer_data['layerAttributes']['linear_before_reset'] = 1 if layer.reset_after and layer.recurrent_activation.__name__ == \"sigmoid\" else 0\n",
    "                        \n",
    "            if layer_data['layerInput'][0].startswith('max_pooling2d'):\n",
    "                pooling_layer_name = layer_data['layerInput'][0].split('/')[0]\n",
    "                layer_data['layerInput'][0] = pooling_layer_name + 'PostTrans'\n",
    "            \n",
    "            fLayerType = layer_data['layerType']\n",
    "            #Ignoring the input layer for models built using Keras Functional API\n",
    "            #NEED TO TEST KERAS FUNCTIONAL API\n",
    "            if(fLayerType == \"InputLayer\"):\n",
    "                continue;\n",
    "\n",
    "            #Adding any required routines depending on the Layer types for generating inference code.\n",
    "            elif (fLayerType == \"Dense\"):\n",
    "                rmodel.AddBlasRoutines({\"Gemm\", \"Gemv\"})\n",
    "            elif (fLayerType == \"BatchNormalization\"):\n",
    "                rmodel.AddBlasRoutines({\"Copy\", \"Axpy\"})\n",
    "            elif (fLayerType == \"Conv1D\" or fLayerType == \"Conv2D\" or fLayerType == \"Conv3D\"):\n",
    "                rmodel.AddBlasRoutines({\"Gemm\", \"Axpy\"})\n",
    "            rmodel = add_layer_into_RModel(rmodel, layer_data)\n",
    "\n",
    "        # Extracting model's weights\n",
    "        weight = []\n",
    "        for idx in range(len(keras_model.get_weights())):\n",
    "            weightProp = {}\n",
    "            weightProp['name'] = keras_model.weights[idx].name\n",
    "            weightProp['dtype'] = keras_model.get_weights()[idx].dtype.name\n",
    "            if 'conv' in keras_model.weights[idx].name and keras_model.weights[idx].shape.ndims == 4:\n",
    "                weightProp['value'] = keras_model.get_weights()[idx].transpose((3, 2, 0, 1)).copy()\n",
    "            else:\n",
    "                weightProp['value'] = keras_model.get_weights()[idx]\n",
    "            weight.append(weightProp)\n",
    "\n",
    "        # Traversing through all the Weight tensors\n",
    "        for weightIter in range(len(weight)):\n",
    "            fWeightTensor = weight[weightIter]\n",
    "            fWeightName = fWeightTensor['name']\n",
    "            fWeightDType = ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fWeightTensor['dtype'])\n",
    "            fWeightTensorValue = fWeightTensor['value']\n",
    "            fWeightTensorSize = 1\n",
    "            fWeightTensorShape = []\n",
    "            \n",
    "            #IS IT BATCH SIZE? CHECK ONNX\n",
    "            if fWeightName.startswith(\"simple_rnn\") or fWeightName.startswith(\"lstm\") or (fWeightName.startswith(\"gru\") and not 'bias' in fWeightName):\n",
    "                fWeightTensorShape.append(1)\n",
    "            \n",
    "            # Building the shape vector and finding the tensor size\n",
    "            for j in range(len(fWeightTensorValue.shape)):\n",
    "                fWeightTensorShape.append(fWeightTensorValue.shape[j])\n",
    "                fWeightTensorSize *= fWeightTensorValue.shape[j]\n",
    "            \n",
    "            if fWeightDType ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "                fWeightArray = fWeightTensorValue\n",
    "                \n",
    "                #weights conversion format between keras and onnx for lstm: the order of the different elements (input, output, forget, cell) inside the vector/matrix is different\n",
    "                if fWeightName.startswith(\"lstm\"):\n",
    "                    if 'kernel' in fWeightName:\n",
    "                        units = int(fWeightArray.shape[1]/4)\n",
    "                        W_i = fWeightArray[:, :units].copy()\n",
    "                        W_f = fWeightArray[:, units: units * 2].copy()\n",
    "                        W_c = fWeightArray[:, units * 2: units * 3].copy()\n",
    "                        W_o = fWeightArray[:, units * 3:].copy()\n",
    "                        fWeightArray[:, units: units * 2] = W_o\n",
    "                        fWeightArray[:, units * 2: units * 3] = W_f\n",
    "                        fWeightArray[:, units * 3:] = W_c\n",
    "                    else: #bias\n",
    "                        units = int(fWeightArray.shape[0]/4)\n",
    "                        W_i = fWeightArray[:units].copy()\n",
    "                        W_f = fWeightArray[units: units * 2].copy()\n",
    "                        W_c = fWeightArray[units * 2: units * 3].copy()\n",
    "                        W_o = fWeightArray[units * 3:].copy()\n",
    "                        fWeightArray[units: units * 2] = W_o\n",
    "                        fWeightArray[units * 2: units * 3] = W_f\n",
    "                        fWeightArray[units * 3:] = W_c\n",
    "            \n",
    "                #need to make specific adjustments for recurrent weights and biases\n",
    "                if (fWeightName.startswith(\"simple_rnn\") or fWeightName.startswith(\"lstm\") or fWeightName.startswith(\"gru\")):\n",
    "                    #reshaping weight matrices for recurrent layers due to keras-onnx inconsistencies\n",
    "                    if 'kernel' in fWeightName:\n",
    "                        fWeightArray = np.transpose(fWeightArray)\n",
    "                        fWeightTensorShape[1], fWeightTensorShape[2] = fWeightTensorShape[2], fWeightTensorShape[1]\n",
    "                    \n",
    "                    fData = fWeightArray.flatten()\n",
    "                    \n",
    "                    #the recurrent bias and the cell bias can be the same, in which case we need to add a vector of zeros for the recurrent bias\n",
    "                    if 'bias' in fWeightName and len(fData.shape) == 1:\n",
    "                        fWeightTensorShape[1] *= 2\n",
    "                        fRbias = fData.copy()*0\n",
    "                        fData = np.concatenate((fData,fRbias))\n",
    "\n",
    "                else:\n",
    "                    fData = fWeightArray.flatten()\n",
    "                    \n",
    "                rmodel.AddInitializedTensor['float'](fWeightName, fWeightTensorShape, fData)\n",
    "            else:\n",
    "                raise TypeError(\"Type error: TMVA SOFIE does not yet support data layer type: \" + fWeightDType)\n",
    "        \n",
    "        # Extracting input tensor info\n",
    "        fPInputs = keras_model.input_names\n",
    "        fPInputShape = keras_model.input_shape if isinstance(keras_model.input_shape, list) else [keras_model.input_shape]\n",
    "        fPInputDType = []\n",
    "        for idx in range(len(keras_model.inputs)):\n",
    "            fPInputDType.append(keras_model.inputs[idx].dtype.__str__()[9:-2])\n",
    "        \n",
    "        if len(fPInputShape) == 1:\n",
    "            fInputName = fPInputs[0]\n",
    "            fInputDType = ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fPInputDType[0])\n",
    "            if fInputDType ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "                if fPInputShape[0][0] is None or fPInputShape[0][0] <= 0:\n",
    "                    fPInputShape = list(fPInputShape[0])\n",
    "                    fPInputShape[0] = 1\n",
    "                rmodel.AddInputTensorInfo(fInputName,  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT, fPInputShape)\n",
    "                rmodel.AddInputTensorName(fInputName) \n",
    "            else:\n",
    "                raise TypeError(\"Type error: TMVA SOFIE does not yet support data type \"+TMVA.Experimental.SOFIE.ConvertStringToType(fInputDType))\n",
    "        else:\n",
    "            #Iterating through multiple input tensors\n",
    "            for fInputName, fInputDType, fInputShapeTuple in zip(fPInputs, fPInputDType, fPInputShape):\n",
    "                fInputDType = ROOT.TMVA.Experimental.SOFIE.ConvertStringToType(fInputDType)\n",
    "                if fInputDType ==  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT:\n",
    "                    if fInputShapeTuple[0] is None or fInputShapeTuple[0] <= 0:\n",
    "                        fInputShapeTuple = list(fInputShapeTuple)\n",
    "                        fInputShapeTuple[0] = 1\n",
    "                        print(\"Model does not have a defined batch size. Assuming it is 1 - input shape: \", fInputShapeTuple)\n",
    "                    rmodel.AddInputTensorInfo(fInputName,  ROOT.TMVA.Experimental.SOFIE.ETensorType.FLOAT, fInputShapeTuple)\n",
    "                    rmodel.AddInputTensorName(fInputName)\n",
    "                else:\n",
    "                    raise TypeError(\"Type error: TMVA SOFIE does not yet support data type \"+TMVA.Experimental.SOFIE.ConvertStringToType(fInputDType))             \n",
    "            \n",
    "        # Adding OutputTensorInfos\n",
    "        outputNames = []\n",
    "        for layerName in keras_model.output_names:\n",
    "            outputNames.append(keras_model.get_layer(layerName).output.name)\n",
    "        rmodel.AddOutputTensorNameList(outputNames)\n",
    "        return rmodel\n",
    "\n",
    "# @pythonization(\"RModelParser_Keras\", ns=\"TMVA::Experimental::SOFIE\")\n",
    "# def pythonize_rmodelparser_keras(klass):\n",
    "#     # Parameters:\n",
    "#     # klass: class to be pythonized \n",
    "#     setattr(klass, \"Parse\", RModelParser_Keras.Parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37f5e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  ReLU_test.h5 in the header  ReLU_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"ReLU_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "750cf39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Tanh_test.h5 in the header  Tanh_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Tanh_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b810310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Softmax_test.h5 in the header  Softmax_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Softmax_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23deecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Sigmoid_test.h5 in the header  Sigmoid_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Sigmoid_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca21dd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Selu_test.h5 in the header  Selu_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Selu_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a3ffa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  LeakyReLU_test.h5 in the header  LeakyReLU_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"LeakyReLU_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "558dab40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Swish_test.h5 in the header  Swish_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Swish_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c57ad98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  BatchNormalization_test.h5 in the header  BatchNormalization_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"BatchNormalization_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8984ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Conv2D_test.h5 in the header  Conv2D_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Conv2D_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2eff447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Flatten_test.h5 in the header  Flatten_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Flatten_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33c8a089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tanh']\n",
      "Sigmoid\n",
      "Generating inference code for the Keras model from  GRU_test.h5 in the header  GRU_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"GRU_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2724589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tanh']\n",
      "Sigmoid\n",
      "Generating inference code for the Keras model from  GRU_with_bias_test.h5 in the header  GRU_with_bias_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"GRU_with_bias_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb0e9c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tanh']\n",
      "Generating inference code for the Keras model from  LSTM_test.h5 in the header  LSTM_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"LSTM_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "346a9b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tanh']\n",
      "Generating inference code for the Keras model from  LSTM_with_bias_test.h5 in the header  LSTM_with_bias_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"LSTM_with_bias_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23cddf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tanh']\n",
      "Generating inference code for the Keras model from  Simple_RNN_test.h5 in the header  Simple_RNN_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Simple_RNN_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "037e4928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tanh']\n",
      "Generating inference code for the Keras model from  Simple_RNN_with_bias_test.h5 in the header  Simple_RNN_with_bias_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Simple_RNN_with_bias_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18085f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  MLP_test.h5 in the header  MLP_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"MLP_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cf69287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  Reshape_test.h5 in the header  Reshape_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Reshape_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79436480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model does not have a defined batch size. Assuming it is 1 - input shape:  [1, 16]\n",
      "Model does not have a defined batch size. Assuming it is 1 - input shape:  [1, 32]\n",
      "Generating inference code for the Keras model from  Add_test.h5 in the header  Add_test.hxx\n"
     ]
    }
   ],
   "source": [
    "modelFile = \"Add_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8cf6e869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating inference code for the Keras model from  MaxPool2D_test.h5 in the header  MaxPool2D_test.hxx\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "none of the 2 overloaded methods succeeded. Full details:\n  void TMVA::Experimental::SOFIE::RModel::Generate(underlying_type_t<TMVA::Experimental::SOFIE::Options> options, int batchSize = -1, long pos = 0, bool verbose = false) =>\n    TypeError: takes at least 1 arguments (0 given)\n  void TMVA::Experimental::SOFIE::RModel::Generate(TMVA::Experimental::SOFIE::Options options = Options::kDefault, int batchSize = -1, int pos = 0, bool verbose = false) =>\n    runtime_error: TMVA SOFIE tensor [max_pooling2dMaxPool0] for which the type is requested is not found, model name: MaxPool2D_test",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m generatedHeaderFile \u001b[38;5;241m=\u001b[39m modelFile\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hxx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating inference code for the Keras model from \u001b[39m\u001b[38;5;124m\"\u001b[39m,modelFile,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the header \u001b[39m\u001b[38;5;124m\"\u001b[39m, generatedHeaderFile)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mrmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m rmodel\u001b[38;5;241m.\u001b[39mOutputGenerated(generatedHeaderFile)\n",
      "\u001b[0;31mTypeError\u001b[0m: none of the 2 overloaded methods succeeded. Full details:\n  void TMVA::Experimental::SOFIE::RModel::Generate(underlying_type_t<TMVA::Experimental::SOFIE::Options> options, int batchSize = -1, long pos = 0, bool verbose = false) =>\n    TypeError: takes at least 1 arguments (0 given)\n  void TMVA::Experimental::SOFIE::RModel::Generate(TMVA::Experimental::SOFIE::Options options = Options::kDefault, int batchSize = -1, int pos = 0, bool verbose = false) =>\n    runtime_error: TMVA SOFIE tensor [max_pooling2dMaxPool0] for which the type is requested is not found, model name: MaxPool2D_test"
     ]
    }
   ],
   "source": [
    "modelFile = \"MaxPool2D_test.h5\"\n",
    "rmodel = RModelParser_Keras.Parse(modelFile)\n",
    "generatedHeaderFile = modelFile.replace(\".h5\",\".hxx\")\n",
    "print(\"Generating inference code for the Keras model from \",modelFile,\"in the header \", generatedHeaderFile)\n",
    "rmodel.Generate()\n",
    "rmodel.OutputGenerated(generatedHeaderFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d1ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
