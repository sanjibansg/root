{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad4a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 12:47:52.238088: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-12 12:47:52.239667: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 12:47:52.278321: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-12 12:47:52.279344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-12 12:47:53.234350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1cbbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 174ms/step - loss: 0.2498 - mae: 0.4787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanna/RootDevelopment/py310/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "x_train = random.rand(10)\n",
    "y_train = random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1,)),  \n",
    "    layers.ReLU()\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.save('ReLU_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e58047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step - loss: 0.1068 - mae: 0.2871\n"
     ]
    }
   ],
   "source": [
    "x_train = random.rand(10)\n",
    "y_train = random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1,)),  \n",
    "    layers.Activation(activation='tanh')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.save('Tanh_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7adaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step - loss: 0.3609 - mae: 0.5070\n"
     ]
    }
   ],
   "source": [
    "x_train = random.rand(10)\n",
    "y_train = random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1,)),  \n",
    "    layers.Activation(activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.save('Softmax_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd7bd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step - loss: 0.1076 - mae: 0.2566\n"
     ]
    }
   ],
   "source": [
    "x_train = random.rand(10)\n",
    "y_train = random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1,)),  \n",
    "    layers.Activation(activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.save('Sigmoid_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19274242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x795c503dcd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2297 - mae: 0.3826\n"
     ]
    }
   ],
   "source": [
    "x_train = random.rand(10)\n",
    "y_train = random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1,)),  \n",
    "    layers.Activation(activation='selu')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.save('Selu_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082d104e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selu\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.__dict__['activation'].__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ec34f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x795c5048f910> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.1435 - mae: 0.3107\n"
     ]
    }
   ],
   "source": [
    "x_train = random.rand(10)\n",
    "y_train = random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1,)),  \n",
    "    layers.Activation(activation='LeakyReLU')       # cannot extract 'alpha' from this type of declaration \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.save('LeakyReLU_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef48147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(0.3, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "for l in model.layers:\n",
    "    pprint.pp(l.__dict__['activation'].alpha)       # .__class__.__name__ gives expected output for LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be4a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 436ms/step - loss: 1.2225 - mae: 0.9517\n"
     ]
    }
   ],
   "source": [
    "x_train = random.rand(10)\n",
    "y_train = random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(1,)),  \n",
    "    layers.BatchNormalization()\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "model.save('BatchNormalization_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7581746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.2503 - mae: 0.4473\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(10, 1, 1)  \n",
    "y_train = np.random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.LSTM(units=10, input_shape = (1, 1))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('LSTM_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca978d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.1311 - mae: 0.2475\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(10, 1, 1)  \n",
    "y_train = np.random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.LSTM(units=10, input_shape = (1, 1), use_bias=True)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('LSTM_with_bias_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c08e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.1884 - mae: 0.3879\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(10, 1, 1)  \n",
    "y_train = np.random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.GRU(10, input_shape = (1, 1))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('GRU_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "780ea7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.2930 - mae: 0.4441\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(10, 1, 1)  \n",
    "y_train = np.random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.GRU(10, input_shape = (1, 1), use_bias=True)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('GRU_with_bias_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13519b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 539ms/step - loss: 0.4345 - mae: 0.5742\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(10, 1, 1)  \n",
    "y_train = np.random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.SimpleRNN(10, input_shape = (1, 1))\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('Simple_RNN_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38052dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 504ms/step - loss: 0.3935 - mae: 0.5680\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(10, 1, 1)  \n",
    "y_train = np.random.rand(10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.SimpleRNN(10, input_shape = (1, 1), use_bias=True)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('Simple_RNN_with_bias_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02dbec79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step - loss: 0.1659 - mae: 0.3339\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(32, 5, 2)\n",
    "y_train = np.random.rand(32, 10)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Flatten()\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('Flatten_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c089d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step - loss: 0.1956 - mae: 0.3614\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(32, 10, 10, 3)\n",
    "y_train = np.random.rand(32, 5, 5, 3)\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.MaxPool2D(pool_size=2)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "model.save('MaxPool_2D_test.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
